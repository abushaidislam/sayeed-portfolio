---
title: 'Cloud Architecture: AWS, Azure, and Multi-Cloud Strategies'
abstract: 'Comprehensive guide to cloud architecture patterns, serverless computing, container orchestration, and multi-cloud deployment strategies for enterprise applications.'
date: '2024-02-12'
banner: /articles/cloud-architecture/banner.jpg
featured: false
---

## Introduction

Cloud architecture has become the foundation of modern scalable applications. This article explores advanced cloud patterns, serverless architectures, and multi-cloud strategies used by enterprise organizations.

## Serverless Architecture with AWS Lambda

```javascript
// Advanced Lambda function with proper error handling
const AWS = require('aws-sdk');
const { v4: uuidv4 } = require('uuid');

const dynamodb = new AWS.DynamoDB.DocumentClient();
const sns = new AWS.SNS();
const s3 = new AWS.S3();

exports.processOrderHandler = async (event, context) => {
  const correlationId = uuidv4();
  
  try {
    console.log(`Processing order with correlation ID: ${correlationId}`, {
      event: JSON.stringify(event),
      requestId: context.awsRequestId
    });

    // Parse SQS message
    const records = event.Records || [];
    const results = [];

    for (const record of records) {
      try {
        const orderData = JSON.parse(record.body);
        const result = await processOrder(orderData, correlationId);
        results.push(result);
        
        // Delete message from queue on success
        await deleteSQSMessage(record.receiptHandle);
        
      } catch (error) {
        console.error(`Failed to process record: ${record.messageId}`, error);
        
        // Send to DLQ or retry based on error type
        if (error.retryable) {
          throw error; // Let SQS retry
        } else {
          await sendToDeadLetterQueue(record, error, correlationId);
        }
      }
    }

    return {
      statusCode: 200,
      body: JSON.stringify({
        message: 'Orders processed successfully',
        correlationId,
        processedCount: results.length
      })
    };

  } catch (error) {
    console.error('Handler error:', error);
    
    // Send alert for critical failures
    await sendAlert({
      severity: 'HIGH',
      message: `Order processing failed: ${error.message}`,
      correlationId,
      requestId: context.awsRequestId
    });

    throw error;
  }
};

async function processOrder(orderData, correlationId) {
  const { orderId, customerId, items, totalAmount } = orderData;
  
  // Validate order data
  if (!orderId || !customerId || !items || items.length === 0) {
    throw new Error('Invalid order data');
  }

  // Start transaction
  const timestamp = new Date().toISOString();
  
  // Save to DynamoDB with conditional check
  const orderRecord = {
    PK: `ORDER#${orderId}`,
    SK: `METADATA`,
    orderId,
    customerId,
    items,
    totalAmount,
    status: 'PROCESSING',
    createdAt: timestamp,
    correlationId,
    ttl: Math.floor(Date.now() / 1000) + (90 * 24 * 60 * 60) // 90 days TTL
  };

  await dynamodb.put({
    TableName: process.env.ORDERS_TABLE,
    Item: orderRecord,
    ConditionExpression: 'attribute_not_exists(PK)',
    ReturnConsumedCapacity: 'TOTAL'
  }).promise();

  // Process inventory updates
  const inventoryUpdates = await updateInventory(items, correlationId);
  
  // Calculate shipping
  const shippingDetails = await calculateShipping(orderData, correlationId);
  
  // Process payment
  const paymentResult = await processPayment({
    customerId,
    amount: totalAmount,
    orderId,
    correlationId
  });

  // Update order status
  await dynamodb.update({
    TableName: process.env.ORDERS_TABLE,
    Key: { PK: `ORDER#${orderId}`, SK: 'METADATA' },
    UpdateExpression: 'SET #status = :status, paymentId = :paymentId, updatedAt = :timestamp',
    ExpressionAttributeNames: { '#status': 'status' },
    ExpressionAttributeValues: {
      ':status': paymentResult.success ? 'CONFIRMED' : 'PAYMENT_FAILED',
      ':paymentId': paymentResult.paymentId,
      ':timestamp': timestamp
    }
  }).promise();

  // Send notification
  if (paymentResult.success) {
    await publishOrderEvent({
      eventType: 'OrderConfirmed',
      orderId,
      customerId,
      correlationId
    });
  }

  return {
    orderId,
    status: paymentResult.success ? 'CONFIRMED' : 'PAYMENT_FAILED',
    correlationId
  };
}

async function publishOrderEvent(eventData) {
  const message = {
    ...eventData,
    timestamp: new Date().toISOString(),
    source: 'order-service'
  };

  await sns.publish({
    TopicArn: process.env.ORDER_EVENTS_TOPIC,
    Message: JSON.stringify(message),
    MessageAttributes: {
      eventType: {
        DataType: 'String',
        StringValue: eventData.eventType
      }
    }
  }).promise();
}
```

## Infrastructure as Code with Terraform

```hcl
# Multi-environment AWS infrastructure
terraform {
  required_version = ">= 1.0"
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
  }
  
  backend "s3" {
    bucket = "company-terraform-state"
    key    = "infrastructure/terraform.tfstate"
    region = "us-west-2"
    encrypt = true
    
    dynamodb_table = "terraform-state-lock"
  }
}

# Variables
variable "environment" {
  description = "Environment name"
  type        = string
  default     = "dev"
}

variable "project_name" {
  description = "Project name for resource naming"
  type        = string
  default     = "ecommerce"
}

# Data sources
data "aws_availability_zones" "available" {
  state = "available"
}

data "aws_caller_identity" "current" {}

# Locals for computed values
locals {
  common_tags = {
    Environment = var.environment
    Project     = var.project_name
    ManagedBy   = "terraform"
  }
  
  name_prefix = "${var.project_name}-${var.environment}"
}

# VPC Configuration
resource "aws_vpc" "main" {
  cidr_block           = "10.0.0.0/16"
  enable_dns_hostnames = true
  enable_dns_support   = true
  
  tags = merge(local.common_tags, {
    Name = "${local.name_prefix}-vpc"
  })
}

# Internet Gateway
resource "aws_internet_gateway" "main" {
  vpc_id = aws_vpc.main.id
  
  tags = merge(local.common_tags, {
    Name = "${local.name_prefix}-igw"
  })
}

# Private and Public Subnets
resource "aws_subnet" "private" {
  count = 2
  
  vpc_id            = aws_vpc.main.id
  cidr_block        = "10.0.${count.index + 1}.0/24"
  availability_zone = data.aws_availability_zones.available.names[count.index]
  
  tags = merge(local.common_tags, {
    Name = "${local.name_prefix}-private-subnet-${count.index + 1}"
    Type = "private"
  })
}

resource "aws_subnet" "public" {
  count = 2
  
  vpc_id                  = aws_vpc.main.id
  cidr_block              = "10.0.${count.index + 10}.0/24"
  availability_zone       = data.aws_availability_zones.available.names[count.index]
  map_public_ip_on_launch = true
  
  tags = merge(local.common_tags, {
    Name = "${local.name_prefix}-public-subnet-${count.index + 1}"
    Type = "public"
  })
}

# NAT Gateway for private subnet internet access
resource "aws_eip" "nat" {
  count = 2
  domain = "vpc"
  
  tags = merge(local.common_tags, {
    Name = "${local.name_prefix}-nat-eip-${count.index + 1}"
  })
}

resource "aws_nat_gateway" "main" {
  count = 2
  
  allocation_id = aws_eip.nat[count.index].id
  subnet_id     = aws_subnet.public[count.index].id
  
  tags = merge(local.common_tags, {
    Name = "${local.name_prefix}-nat-gateway-${count.index + 1}"
  })
  
  depends_on = [aws_internet_gateway.main]
}

# EKS Cluster
resource "aws_eks_cluster" "main" {
  name     = "${local.name_prefix}-cluster"
  role_arn = aws_iam_role.eks_cluster.arn
  version  = "1.28"
  
  vpc_config {
    subnet_ids              = concat(aws_subnet.private[*].id, aws_subnet.public[*].id)
    endpoint_private_access = true
    endpoint_public_access  = true
    public_access_cidrs     = ["0.0.0.0/0"]
  }
  
  encryption_config {
    provider {
      key_arn = aws_kms_key.eks.arn
    }
    resources = ["secrets"]
  }
  
  enabled_cluster_log_types = ["api", "audit", "authenticator", "controllerManager", "scheduler"]
  
  tags = local.common_tags
  
  depends_on = [
    aws_iam_role_policy_attachment.eks_cluster_policy,
    aws_iam_role_policy_attachment.eks_service_policy,
    aws_cloudwatch_log_group.eks_cluster
  ]
}

# EKS Node Group
resource "aws_eks_node_group" "main" {
  cluster_name    = aws_eks_cluster.main.name
  node_group_name = "${local.name_prefix}-nodes"
  node_role_arn   = aws_iam_role.eks_node_group.arn
  subnet_ids      = aws_subnet.private[*].id
  
  instance_types = ["t3.medium"]
  ami_type       = "AL2_x86_64"
  capacity_type  = "ON_DEMAND"
  
  scaling_config {
    desired_size = 2
    max_size     = 10
    min_size     = 1
  }
  
  update_config {
    max_unavailable = 1
  }
  
  tags = local.common_tags
  
  depends_on = [
    aws_iam_role_policy_attachment.eks_worker_node_policy,
    aws_iam_role_policy_attachment.eks_cni_policy,
    aws_iam_role_policy_attachment.eks_container_registry_policy
  ]
}

# RDS Database
resource "aws_db_subnet_group" "main" {
  name       = "${local.name_prefix}-db-subnet-group"
  subnet_ids = aws_subnet.private[*].id
  
  tags = merge(local.common_tags, {
    Name = "${local.name_prefix}-db-subnet-group"
  })
}

resource "aws_db_instance" "main" {
  identifier = "${local.name_prefix}-database"
  
  engine         = "postgres"
  engine_version = "15.4"
  instance_class = var.environment == "prod" ? "db.r6g.xlarge" : "db.t3.micro"
  
  allocated_storage     = var.environment == "prod" ? 100 : 20
  max_allocated_storage = var.environment == "prod" ? 1000 : 100
  
  db_name  = "ecommerce"
  username = "dbadmin"
  password = random_password.db_password.result
  
  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.main.name
  
  backup_retention_period = var.environment == "prod" ? 30 : 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"
  
  storage_encrypted = true
  kms_key_id       = aws_kms_key.rds.arn
  
  skip_final_snapshot       = var.environment != "prod"
  final_snapshot_identifier = var.environment == "prod" ? "${local.name_prefix}-final-snapshot" : null
  
  tags = local.common_tags
}

# Redis ElastiCache
resource "aws_elasticache_subnet_group" "main" {
  name       = "${local.name_prefix}-cache-subnet"
  subnet_ids = aws_subnet.private[*].id
}

resource "aws_elasticache_replication_group" "main" {
  replication_group_id       = "${local.name_prefix}-redis"
  description                = "Redis cluster for ${local.name_prefix}"
  
  port               = 6379
  parameter_group_name = "default.redis7"
  node_type          = var.environment == "prod" ? "cache.r6g.large" : "cache.t3.micro"
  
  num_cache_clusters = var.environment == "prod" ? 3 : 1
  
  subnet_group_name  = aws_elasticache_subnet_group.main.name
  security_group_ids = [aws_security_group.redis.id]
  
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true
  auth_token                = random_password.redis_password.result
  
  tags = local.common_tags
}

# Outputs
output "cluster_endpoint" {
  description = "EKS cluster endpoint"
  value       = aws_eks_cluster.main.endpoint
}

output "database_endpoint" {
  description = "RDS database endpoint"
  value       = aws_db_instance.main.endpoint
  sensitive   = true
}

output "redis_endpoint" {
  description = "Redis cluster endpoint"
  value       = aws_elasticache_replication_group.main.primary_endpoint_address
  sensitive   = true
}
```

## Container Orchestration with Kubernetes

```yaml
# Complete application deployment
apiVersion: v1
kind: Namespace
metadata:
  name: ecommerce-app
  labels:
    name: ecommerce-app

---
# ConfigMap for application configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: app-config
  namespace: ecommerce-app
data:
  DATABASE_HOST: "postgres.example.com"
  REDIS_HOST: "redis.example.com"
  LOG_LEVEL: "info"
  METRICS_PORT: "9090"

---
# Secret for sensitive data
apiVersion: v1
kind: Secret
metadata:
  name: app-secrets
  namespace: ecommerce-app
type: Opaque
data:
  DATABASE_PASSWORD: <base64-encoded-password>
  REDIS_PASSWORD: <base64-encoded-password>
  JWT_SECRET: <base64-encoded-secret>

---
# Deployment for web application
apiVersion: apps/v1
kind: Deployment
metadata:
  name: web-app
  namespace: ecommerce-app
  labels:
    app: web-app
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  selector:
    matchLabels:
      app: web-app
  template:
    metadata:
      labels:
        app: web-app
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "9090"
    spec:
      serviceAccountName: web-app-sa
      securityContext:
        runAsNonRoot: true
        runAsUser: 1000
        fsGroup: 2000
      containers:
      - name: web-app
        image: mycompany/web-app:v1.2.0
        imagePullPolicy: Always
        ports:
        - containerPort: 8080
          name: http
        - containerPort: 9090
          name: metrics
        env:
        - name: PORT
          value: "8080"
        envFrom:
        - configMapRef:
            name: app-config
        - secretRef:
            name: app-secrets
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /health/live
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
          timeoutSeconds: 5
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /health/ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
          timeoutSeconds: 3
          failureThreshold: 3
        volumeMounts:
        - name: tmp
          mountPath: /tmp
        - name: logs
          mountPath: /app/logs
      volumes:
      - name: tmp
        emptyDir: {}
      - name: logs
        emptyDir: {}

---
# Service for web application
apiVersion: v1
kind: Service
metadata:
  name: web-app-service
  namespace: ecommerce-app
  labels:
    app: web-app
spec:
  selector:
    app: web-app
  ports:
  - port: 80
    targetPort: 8080
    name: http
  - port: 9090
    targetPort: 9090
    name: metrics
  type: ClusterIP

---
# Ingress for external access
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web-app-ingress
  namespace: ecommerce-app
  annotations:
    kubernetes.io/ingress.class: "nginx"
    cert-manager.io/cluster-issuer: "letsencrypt-prod"
    nginx.ingress.kubernetes.io/rate-limit: "100"
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
spec:
  tls:
  - hosts:
    - app.example.com
    secretName: web-app-tls
  rules:
  - host: app.example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: web-app-service
            port:
              number: 80

---
# HorizontalPodAutoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: web-app-hpa
  namespace: ecommerce-app
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: web-app
  minReplicas: 3
  maxReplicas: 10
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

## Multi-Cloud Strategy

```javascript
// Multi-cloud abstraction layer
class CloudProvider {
  constructor(config) {
    this.config = config;
  }

  // Abstract methods to be implemented by each provider
  async deployFunction(functionConfig) {
    throw new Error('Method must be implemented');
  }

  async createDatabase(dbConfig) {
    throw new Error('Method must be implemented');
  }

  async setupStorage(storageConfig) {
    throw new Error('Method must be implemented');
  }
}

class AWSProvider extends CloudProvider {
  constructor(config) {
    super(config);
    this.lambda = new AWS.Lambda(config.aws);
    this.rds = new AWS.RDS(config.aws);
    this.s3 = new AWS.S3(config.aws);
  }

  async deployFunction(functionConfig) {
    const params = {
      FunctionName: functionConfig.name,
      Runtime: functionConfig.runtime || 'nodejs18.x',
      Role: functionConfig.roleArn,
      Handler: functionConfig.handler,
      Code: {
        ZipFile: functionConfig.code
      },
      Environment: {
        Variables: functionConfig.environment || {}
      },
      Timeout: functionConfig.timeout || 30,
      MemorySize: functionConfig.memory || 128
    };

    try {
      const result = await this.lambda.createFunction(params).promise();
      return {
        provider: 'aws',
        functionArn: result.FunctionArn,
        endpoint: result.FunctionArn
      };
    } catch (error) {
      if (error.code === 'ResourceConflictException') {
        // Update existing function
        const updateResult = await this.lambda.updateFunctionCode({
          FunctionName: functionConfig.name,
          ZipFile: functionConfig.code
        }).promise();
        return {
          provider: 'aws',
          functionArn: updateResult.FunctionArn,
          endpoint: updateResult.FunctionArn
        };
      }
      throw error;
    }
  }

  async createDatabase(dbConfig) {
    const params = {
      DBInstanceIdentifier: dbConfig.name,
      DBInstanceClass: dbConfig.instanceType || 'db.t3.micro',
      Engine: dbConfig.engine || 'postgres',
      MasterUsername: dbConfig.username,
      MasterUserPassword: dbConfig.password,
      AllocatedStorage: dbConfig.storage || 20,
      VpcSecurityGroupIds: dbConfig.securityGroups || [],
      DBSubnetGroupName: dbConfig.subnetGroup
    };

    const result = await this.rds.createDBInstance(params).promise();
    return {
      provider: 'aws',
      identifier: result.DBInstance.DBInstanceIdentifier,
      endpoint: result.DBInstance.Endpoint?.Address,
      port: result.DBInstance.Endpoint?.Port
    };
  }
}

class AzureProvider extends CloudProvider {
  constructor(config) {
    super(config);
    // Initialize Azure SDKs
    this.functionApp = new Azure.WebSiteManagementClient(config.azure);
    this.sqlManagement = new Azure.SqlManagementClient(config.azure);
    this.storageClient = new Azure.StorageManagementClient(config.azure);
  }

  async deployFunction(functionConfig) {
    // Azure Functions deployment logic
    const functionApp = {
      location: functionConfig.region || 'East US',
      kind: 'functionapp',
      properties: {
        serverFarmId: functionConfig.appServicePlan,
        siteConfig: {
          appSettings: Object.entries(functionConfig.environment || {}).map(
            ([key, value]) => ({ name: key, value })
          )
        }
      }
    };

    const result = await this.functionApp.webApps.createOrUpdate(
      functionConfig.resourceGroup,
      functionConfig.name,
      functionApp
    );

    return {
      provider: 'azure',
      functionId: result.id,
      endpoint: `https://${functionConfig.name}.azurewebsites.net`
    };
  }
}

// Multi-cloud deployment orchestrator
class MultiCloudDeployment {
  constructor(providers) {
    this.providers = providers;
  }

  async deployApplication(appConfig) {
    const deploymentResults = {};

    for (const [providerName, config] of Object.entries(appConfig.providers)) {
      const provider = this.providers[providerName];
      
      if (!provider) {
        throw new Error(`Provider ${providerName} not configured`);
      }

      try {
        console.log(`Deploying to ${providerName}...`);
        
        const results = await Promise.all([
          this.deployFunctions(provider, config.functions || []),
          this.deployDatabases(provider, config.databases || []),
          this.deployStorage(provider, config.storage || [])
        ]);

        deploymentResults[providerName] = {
          functions: results[0],
          databases: results[1],
          storage: results[2],
          status: 'success'
        };

      } catch (error) {
        console.error(`Deployment to ${providerName} failed:`, error);
        deploymentResults[providerName] = {
          status: 'failed',
          error: error.message
        };
      }
    }

    return deploymentResults;
  }

  async deployFunctions(provider, functions) {
    const results = [];
    
    for (const functionConfig of functions) {
      const result = await provider.deployFunction(functionConfig);
      results.push(result);
    }
    
    return results;
  }

  // Health check across providers
  async checkHealth() {
    const healthStatus = {};

    for (const [providerName, provider] of Object.entries(this.providers)) {
      try {
        const health = await provider.healthCheck();
        healthStatus[providerName] = {
          status: 'healthy',
          ...health
        };
      } catch (error) {
        healthStatus[providerName] = {
          status: 'unhealthy',
          error: error.message
        };
      }
    }

    return healthStatus;
  }
}

// Usage example
const multiCloud = new MultiCloudDeployment({
  aws: new AWSProvider({ aws: { region: 'us-west-2' } }),
  azure: new AzureProvider({ azure: { subscriptionId: 'xxx' } })
});

const appConfig = {
  providers: {
    aws: {
      functions: [{
        name: 'order-processor',
        runtime: 'nodejs18.x',
        handler: 'index.handler',
        code: functionZip,
        environment: { NODE_ENV: 'production' }
      }],
      databases: [{
        name: 'orders-db',
        engine: 'postgres',
        instanceType: 'db.t3.micro'
      }]
    },
    azure: {
      functions: [{
        name: 'backup-processor',
        runtime: 'node',
        resourceGroup: 'backup-rg'
      }]
    }
  }
};

await multiCloud.deployApplication(appConfig);
```

## Conclusion

Modern cloud architecture requires understanding multiple platforms and patterns:

- **Serverless Computing**: Event-driven, auto-scaling functions
- **Infrastructure as Code**: Reproducible, version-controlled infrastructure
- **Container Orchestration**: Scalable, resilient application deployment
- **Multi-Cloud Strategy**: Vendor independence and risk mitigation

Success in cloud architecture comes from choosing the right tools for each use case and maintaining operational excellence across all environments.